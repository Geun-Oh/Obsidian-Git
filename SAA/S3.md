> [!info] Simple Storage Service


>[!summary] S3 Summary
>
>- key - value 객체 저장소
>- 큰 규모의 객체 저장에 유용하지만, 많은 작은 객체 저장에는 유용하지 않다.
>- 각 객체의 최대 크기는 5TB이며, 서버리스, 무한 확장성, 버저닝 가능성 등의 특징이 있다.
>  - 계층(tier): S3 Standard / S3 Infrequent Access / S3 Intelligent /  S3 Glacier + lifecycle 정책
>- 게층 전환을 하려면 수명 주기 정ㅊ

# 1. Amazon S3

S3는 AWS의 주요 구성 중 하나이며 무한하게 확장할 수 있는 스토리지라고 설명한다 많은 웹이 S3를 활용하고 있으며 많은 AWS 서비스 자체도 아마존 S3를 통합을 위해 사용한다

## 1.1. S3 use cases

- 백업과 스토리지로 사용
- 재해 복구의 용도
- 아카이브용 -> 추후 매우 손쉽게 검색
- 하이브리드 클라우드 스토리지
- 동영상 파일이나 이미지등 미디어 호스트
- 대규모 데이터 분석
- 소프트웨어 배포
- 정적 웹사이트 호스팅(나스닥은 7년간 데이터를 S3의 저장)

## 1.2. S3 - buckets

- S3는 파일을 버킷에 저장하는데 버킷은 상위 레벨 디렉토리로 표시됨
- **S3 버킷의 파일은 객체라고하며 버킷은 모든 계정, 리전을 포함하여 고유한 이름을 가져야 한다**
- 버킷은 리전 레벨에서 정의됨
- 전역 서비스처럼 보이지만 리전에 따라 생성
- 네이밍컨벤션
    - 대문자 밑줄 불가
    - 3~63자
    - ip 주소불가
    - 소문자 숫자
    - xn 시작 불가
    - -3alias 마무리 불가
    - 소문자, 숫자, 하이픈만 사용하면 된다

## 1.3. Objects

- 객체는 키를 가진다
- 키는 객체가 있는 전체 경로를 의미하며, 폴더에 중첩되는 경우 상위 경로에 포함된다
    - s3://my-bucket/my_file.txt
    - s3://my-bucket/my_folder1/another_folder/my_file.txt
- 키는 접두사와 객체 이름으로 구성
    - • s3://my-bucket/my_folder1/another_folder/my_file.txt
    - 마지막엔 반드시 객체 이름이 온다
- S3 그 자체로는 디렉토리의 개념은 없다.
    - 다만 콘솔이나 UI를 보게 되면 다르게 생각하고 디렉토리를 만든다
- 핵심은 키이며 슬래시를 포함하고 접두사와 객체 이름으로 만들어진다
- 객체의 값은 본문의 내용
- 최대 크기 5TB, 매우 큰 파일을 업로드하는 경우 멀티파트 업로드를 통해 나눠야한다.
    - 100MB 이상의 파일을 업로드할 경우 멀티파트 업로드 필요
- 메타데이터(객체의 키 - 값 쌍 리스트) - 파일에 관한 요소나 메타 데이터
    - 태그 → 유니코드 키 값 쌍으로 최대 10개까지 가능하며 보안, 라이프사이클에 유용함
- 버전관리를 활성화한 경우 버전 아이디가 붙는다

# 2. 실습

Bucket name은 고유함.

버킷은 글로벌 서비스가 아닌 특정 리전에 할당

![](https://blog.kakaocdn.net/dn/teu8I/btshG7gElUs/lhqGtkU1KC7hAh6RTaZa7K/img.png)

공개 URL을 통해 접근하는 경우 안될수도 있다. → 설정이 필요함

서명된 URL이 필요한데 AWS에서 부여

일반적인 클라우드 스토리지 서비스와 UX가 매우 유사해서 어려운점은 없다

# 3. S3 보안

## 3.1. User-Based

- IAM 정책 - 특정 사용자에게 허용되어야 하는 API를 지정하거나 승인함

## 3.2. Resource-Based

- S3 버킷 정책 - S3 콘솔에서 직접 할당할 수 있는 넓은 버킷 규칙으로 특정 사용자를 허용하거나 다른 계정의 사용자를 허용할 수 있다
    - 이를 버킷에 엑세스할 수 있는 Cross Account라고 함
    - 동시에 S3 버킷을 공개로 만드는 방법이기도 함
- Object Access Control List (ACL) - 보다 세밀한 규칙 (비활성화 가능)
- Bucket Access Control List (ACL) - 버킷 수준에서 할 수 있는 규칙으로 OACL보다 일반적(비활성화 가능)

가장 일반적인 방법은 버킷 정책을 사용하는 것이다.

그렇다면 어떤 상황에서 IAM 정책이 S3 객체로 엑세스 할 수 있는 경우인가

- IAM 권한이 허용하거나 리소스 정책이 허용하는 경우
- 엑세스에 명확한 거부가 없는 경우

## 3.3. Encryption

- 암호키를 사용하여 객체를 암호화하는 방법

# 4. S3 Bucket Policies (버킷 정책)

![](https://blog.kakaocdn.net/dn/HhKWn/btshBjIOgrh/ipo41bYW9ivwQBv4Kfbhyk/img.png)

- JSON 기반의 정책
    - Resources: 정책이 적용되는 버킷과 객체를 알려줌
    - Effect: 허용/거부 (Action을 거부하거나 허용하는 방식)
    - Action: 허용하거나 거부할 수 있는 작업의 집합
    - Principal: 정책을 적용할 계정과 사용자를 정의
- S3 버킷 정책의 의미
    - 버킷에 대한 퍼블릭 엑세스 권한을 부여 가능
    - 업로드시 객체를 강제 암호화
    - 다른 계정이 엑세스 할 수 있도록 하는 권한을 준다

# 5. Bucket settings for Block Public Access

![](https://blog.kakaocdn.net/dn/brFzMq/btshHMQYVqW/Dq3dfnnxpqKxKlKB44Cydk/img.png)

- 버킷을 생성할 때 설정한 것, 기업 데이터 유출을 방지하기 위해 AWS가 개발한 추가 보안 계층
- S3 버킷 정책을 설정하여 공개로 만들어도 옵션이 활성화되어 있으면 절대 공개되지 않음
    - 공개하면 안된다면 이 버킷을 계속해서 설정해놓으면 됨
- 계정 수준에서 설정이 가능하다

# 6. Static Website Hosting

- S3에서 정적 웹사이트 호스팅과 인터넷 접근을 가능하게 함
- URL은 리전에 따라 달라짐
- public read 정책을 허용해야 함

# 7. Bucket Versioning

- 파일을 버전 관리 가능하며 버킷 수준에서 활성화해야함
- 동일한 키로 파일을 덮어쓰면 새로운 버전을 생성한다
- 의도하지 않게 삭제하지 않도록 보호해서 이전 버전을 복구 가능
- 이전 버전으로 쉽게 롤백할 수 있다

## 7.1. 주의 사항

- 버전 관리를 활성화 하기전에 버전 관리가 적용되지 않은 모든 파일은 null 버전을 갖게 된다
- 버전 관리를 중단해도 이전 버전을 삭제하지 않는다

![](https://blog.kakaocdn.net/dn/bxPQxg/btshAq9QICq/TV2PBa1vJ05AjFBTKSOJd0/img.png)

## 7.2. Versioning 실습

![](https://blog.kakaocdn.net/dn/bPs6fJ/btshBjB51A2/mKt33E8K9GVWJuF9ic7e0K/img.png)

# 8. S3 복제 - Replication (CRR & SRR)

Cross-Region Replication (CRR): 교차 리전 복제

Same-Region Replication (SRR): 같은 리전으로 복제

- 소스 버킷과 복제 대상 버킷 둘 모두 버전 관리 기능이 활성화되어야 한다
- CRR은 리전이 달라야하고 SRR은 같아야함
- 복제는 비동기식으로 이루어지며 다른 AWS 계정 간에도 버킷 복제가 허용됨
- 복제 과정은 백그라운드에서 이루어짐
- S3에 올바른 읽기 쓰기 권한이 필요
- 사용 사례
    - CRR의 경우 법규 혹은 내부 체제 관리, 다른 리전에 있어 발생하는 지연 시간 줄이기, 계정 간 복제
    - SRR의 경우 다수의 S3 버킷간 로그 통합, 개발환경이 별도로 있어 운영 환경과 개발 환경간의 테스트를 위한 실시간 복제가 필요한 경우

## 8.1. 복제 시 주의사항

- 복제를 활성화 한 경우에는 이후 새로 추가하는 객체만 복제 대상
- 기존의 객체를 복제하려면 S3 배치 복제를 사용해야 한다
    - 배치복제는 기존 객체부터 복제에 실패한 객체까지 복제할 수 있는 기능
- 작업을 삭제
    - 소스 버킷에서 대상 버킷으로 삭제 마커를 복제하면 됨(optional)
    - 버전 ID로 삭제하는 경우 버전 ID는 복제되지 않아 다른 버킷에서 삭제 불가
- 체이닝 복제는 불가능

# 9. 버킷 객체 복제 실습

- 기본적으로 삭제 마커 복제는 활성화 되지 않지만 해당 옵션을 활성화하면 삭제 마커도 한 버킷에서 다른 버킷으로 복제되니 주의
- 또 주의할 점은 오리진 버킷에서 객체를 영구 삭제해도 복제 버킷에서는 삭제되지 않는다

# 10. S3 Storage Classes

- Amazon S3 Standard - General Purpose
- Amazon S3 Standard-Infrequent Access (IA)
- Amazon S3 One Zone-Infrequent Access
- Amazon S3 Glacier Instant Retrieval
- Amazon S3 Glacier Flexible Retrieval
- Amazon S3 Glacier Deep Archive
- Amazon S3 Intelligent Tiering

→ S3에서 객체를 생성할 때 클래스를 선택할 수 있고 수동으로 수정할수도 있다

→ 혹은 수명주기를 활용해 스토리지 클래스 간에 객체를 자동으로 이동할 수도 있다

## 10.1. S3 Durability and Availability

- Durablilty 내구성
    - S3로 인해 객체가 손실되는 횟수로 AWS는 뛰어난 내구성을 제공한다
    - 천만 개당 평균 10,000년에 한번씩 손실
    - 모든 스토리지 클래스의 내굿어은 동일
- Availability 가용성
    - 서비스가 얼마나 용이하게 제공되는지를 나타낸다
    - 스토리지 클래스에 따라 다랄진다
    - 애플리케이션을 개발할 때 고려해야 한다

## 10.2. S3 Standard – General Purpose

- 99.99% 가용성
- 자주 엑세스하는 데이터에 사용
- 지연시간이 짧고 처리량이 높다
- 두 개의 기능 장애를 동시에 버틸 수 있다
- 빅데이터 분석, 모바일 혹은 게임 애플리케이션, 콘텐츠 배포에서 사용

## 10.3. S3 Storage Classes – Infrequent Access

- 자주 엑세스하진 않지만 빠른 엑세스가 필요한 데이터
- 스탠다드보단 저렴하지만 검색 비용이 청구된다

### 10.3.1. Amazon S3 Standard-Infrequent Access (S3 Standard-IA)

99.9% 높은 가용성, 그리고 재해 복구와 백업

### 10.3.2. Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)

단일 AZ 내에서는 높은 내구성을 갖지만 AZ가 파괴된 경우 데이터를 잃는다

99.5%의 가용성과 온프레미스 데이터의 보조 백업 복사본, 재생성 가능한 데이터 저장에서 사용

## 10.4. Amazon S3 Glacier Storage Classes

- 아카이빙과 백업을 위한 저비용 객체 스토리지이다
- 비용에는 스토리지 비용과 검색 비용

### 10.4.1. Amazon S3 Glacier Instant Retrieval

밀리초 단위로 검색 가능하며 분기마다 한 번 엑세스하는 데이터에 적합하다

최소 보관 기간이 90일이고 백업이지만 밀리초 이내에 엑세스 해야하는 경우 적합하다

### 10.4.2. Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier)

티어가 추가되면서 이름이 바뀜

- 데이터를 받는데 걸리는 시간
    - Expedited → 1~5 min
    - Standard → 3~5 hours
    - Bulk → 5~12 hours, free
- 최소 보관기간 90일

### 10.4.3. Amazon S3 Glacier Deep Archive – for long term storage

- Standard → 12 hours
- Bulk → 48 hours

가장 저렴한 비용과 180일의 최소 보관 기간

## 10.5. S3 Intelligent-Tiering

- 사용 패턴에 따라 자동으로 허가된 티어 간 객체를 이동할 수 있게 해준다
- 소액의 월별 모니터링 비용과 티어링 비용이 발생한다
- 검색 비용은 없다

![](https://blog.kakaocdn.net/dn/GRNDi/btshBjB52q1/PgbPqFSGQioDBk7qS0uBx0/img.png)

수치는 중요하지 않고 내용만 이해하면 된다

![](https://blog.kakaocdn.net/dn/biNMGc/btshCdPekTC/ue33OigrnKH1n7T0sTYhd0/img.png)



---


# 1. 스토리지 클래스간 데이터 이동

- 스토리지 클래스 간에 객체를 이동시킬 수 있다
- 자신보다 아래있는 계층으로 이동할 수 있다
- 수동으로 옮기거나 라이프사이클을 활용하여 자동으로 옮길 수 있다

각 클래스가 무슨 역할을 하는지 이해하면 됨

![](https://blog.kakaocdn.net/dn/bqPAte/btshCbKyA8o/R2SjQzbahsuyrwWtoXhOVk/img.png)

# 2. Lifecycle Rules

- **Transition Actions**(전환 작업) - 다른 스토리지 클래스로 객체를 전환하도록 구성
    - 60일 후에 Standard IA로 옮기도록 설정 등
- **Expiration Actions**(만료 작업) - 일정 시간이 지나면 객체가 삭제 또는 만료되도록 설정
    - 365일 후에 로그 파일을 삭제
    - 한달 후 이전 버전의 파일을 삭제
    - 완료되지 않은 멀티파트 업로드 부분을 삭제

특정 접두사를 사용해 Rules를 전체 혹은 일부 경로에만 적용할 수 있고 특정 객체 태그에만 지정 가능

### 2.1.1. 가상의 시나리오 → 이런식으로 문제가 출제된다는 사실을 인지하면 된다

![](https://blog.kakaocdn.net/dn/LzrYQ/btshMWyZyKz/12I8YltYpUWBJb4rFCgsfK/img.png)

썸네일 이미지는 엑세스가 많지 않고 재생성이 쉬움

소스 이미지는 액세스가 많고 재생성이 어렵다

- 소스 이미지를 스탠다드로 설정하다가 라이프 사이클 설정을 통해60일 뒤해 Glacier로 전환
- 썸네일은 One Zone IA로 두고 60일 이후 만료 혹은 삭제 시킨다
- 썸네일과 소스 이미지는 접두사를 통해 구분시킬 수 있음

![](https://blog.kakaocdn.net/dn/b7Giu0/btshKREAqt2/o2Ds4lLaWpai8pm52QdZUK/img.png)

30일 이내엔 객체 삭제시 즉시 복구, 365 이내에는 48시간내에 복구할 수 있도록 설계

- 우선 S3 Versioning을 통해 삭제 마커를 활용해야 한다
- 객체의 이전 버전을 Standard IA로 옮김 → 최상위 버전엔 두지 않는다
- 그런 다음 Glacier Deep Archive로 옮겨야 함

# 3. Storage Class Analysis

객체를 전환하는데 최적의 기간을 정하는 방법

- S3 Analytics를통해 적합한 스토리지 클래스로 전환할 때 사용할 수 있음
- Standard와 Standard IA용 권장사항을 제공
    - One Zone IA or Glacier은 해당 없다
- S3 Analytics를 사용하면 권장사항과 통계를 csv 파일로 제공한다
- 보고서는매일 업데이트 되며 24~48시간까지 데이터 분석 시간이 걸림

이를 통해 개선 방안을 찾을 수 있다

![](https://blog.kakaocdn.net/dn/W9iWe/btshCdn4x9K/yZM3kGEwHFV3ks7YP4UYM0/img.png)

# 4. S3 - Requester Pays (요청자 지불)

버킷 소유자가 버킷과 관련된 모든 Amazon S3 스토리지 및 데이터 전송 비용을 지불한다

- 요청자가 데이터 다운로드 비용을 지불
- 대량의 데이터셋을 다른 계정과 공유하려는 경우 유용하다
    - 요청자가 익명이어서는 안되며 AWS 인증을 받아야함

![](https://blog.kakaocdn.net/dn/dvK2GB/btshy8BOtBU/b2voNAUA5LVVkjIogfnXw0/img.png)

# 5. S3 Event Notifications

- 객체가 생성된 경우, 삭제, 복원, 복제가 발생된 경우 이벤트가 발생함
- 이벤트 필터링 가능(.jpg로 발생한 이벤트)
- S3 특정 이벤트에 자동으로 반응하게 할 수 있다
- 예시) 업로드 되는 사진의 섬네일을 만드려고 하면 이벤트 알림을 만들어서 설정 가능
- 원하는 만큼의 이벤트 생성과 어디로든 보낼 수 있다

![](https://blog.kakaocdn.net/dn/lpa3T/btshA3TXjcN/qvww1ktGWNvtkwiyKOog30/img.png)

## 5.1. S3 Event Notifications with Amazon EventBridge

위 3개 말고도 최근 **Amazon EventBridge**와 통합됐다

![](https://blog.kakaocdn.net/dn/bIVT8v/btshCfM3IBN/MtRsafQmedJlK1CDM8yo6k/img.png)

- 이벤트가 S3 버켓으로 이동하면 종류와 상관없이 모든 이벤트는 EventBridge를 거친다
- 여기서 Event Rule을 설정할 수 있고 룰에 따라 18개가 넘는 AWS 서비스에 알람을 보낼 수 있다
- 고급 필터링 옵션을 더 많이 설정 가능하고 더 많은 목적지, 많은 기능을 활용 가능

중요한 점은 Amazone S3에서 발생하는 모든 이벤트에 반응할 수 있다

# 6. 실습

S3의 이벤트 알림을 받는다는 설정을 SQS, Lamda 등 각 AWS 서비스에서 허용 설정을 해야 한다

# 7. Baseline Performance (기준 성능)

- S3는 요청이 아주 많을 때 자동으로 확장된다
- S3로부터 첫 번째 바이트를 수신하는데 지연시간도 100~200밀리초 사이로 아주 빠르다
- 버킷 내 접두사(prefix)당 초당 적어도 3,500개의 PUT/COPY/POST/DELETE 요청 또는 5,500개의 GET/HEAD 요청을 지원한다
- 버킷 내에서 접두사 수에 제한은 없다
- prefix 예시
    - file이라는 객체와 bucket사이에 모든 것이 접두사가 된다
    - bucket/folder1/sub1/file → /folder1/sub1/
    - bucket/folder1/sub2/file → /folder1/sub2/
    - bucket/1/file → /1/
    - bucket/2/file → /2/
- 위 접두사 예시에 읽기 요청을 균등하게 처리할 수 있으면 GET/HEAD 요청을 초당 22,000개를 처리할 수 있다

# 8. S3 Performance - 성능과 최적화 방법

- 100MB를 넘는 파일은 멀티-파트 업로드 하는게 좋고 5GB가 넘으면 필수
- 멀티파트 업로드는 업로드를 병렬화하기 떄문에 전송 속도를높여 대역폭을 최대화 할 수 있다

![](https://blog.kakaocdn.net/dn/nwEUl/btshCd9z8Yh/CVVIubbb8fjwAd1TTe8KIK/img.png)

## 8.1. S3 Transfer Acceleration - 업로드와 다운로드를 위한 전송 가속화

- 공용 인터넷으로 연결되는 경우 AWS 엣지 로케이션으로 전송하여 전송속도를 높이고 이후 데이터를 프라이빗 aws 네트워크를 통해 대상 리전에 있는 S3 버킷으로 전달
    - 엣지 로케이션은 리전보다 훨씬 많다 약 200개로 계속 증가하고 있다
- 멀티파트 업로드와 같이 사용할 수 있다
- 공용 인터넷을 최소화하고 프라이빗 AWS 네트워크 사용량을 늘림으로써 전송속도 최적화

![](https://blog.kakaocdn.net/dn/uNDDH/btshE0IQLYF/Q8nqHjIorPKHYPQ4t68B00/img.png)

# 9. S3 Byte-Range Fetches 파일을 수신하고 읽는 효율적인 방법

- 특정 바이트 범위를 요청하여 Get 요청을 병렬화
- 특정 바이트 범위를 가져오는데 실패한 경우 더 작은 단위로 재시도
    - 실패 한다고 해도 복원력이 높다
- 다운로드 속도를높일 수 있다

![](https://blog.kakaocdn.net/dn/cXHOAe/btshAzFxfIO/7Kv8yPHoOCCowoFtDhvZy1/img.png)

파일의 일부분만 가져와 파일의 정보를 읽을 수 있다

![](https://blog.kakaocdn.net/dn/KwqCG/btshzASlPyp/cFPLJqZTIQ7IxMWSMvEBR1/img.png)

# 10. S3 Select & Glacier Select

s3에서 파일을 검색할 때 검색한 다음 필터링하면 너무 많은 데이터를 검색하게 된다

- Server-side filtering을 통해 더 적은 수의 데이터를 검색할 수 있고 이는 성능 향상을 가져옴
- SQ문에서 간단히 행과열을 사용해 필터링 가능
- 네트워크 전송이 줄기 때문에 데이터 검색과 필터링에 드는 클라이언트 측의 CPU 비용도 줄어든다
- S3 Select 이전에는 모든 데이터를 검색한 후 애플리케이션에서 필터링을 통해 필요한 찾음
- S3 select를 사용하면 Amazon S3가 대신 파일을 필터링 해주고 필요한 데이터만 검색할 수 있다

간단한 필터링에 **S3 Select & Glacier Select**를 사용하면 좋다

![](https://blog.kakaocdn.net/dn/7Trgp/btshKRYTq0N/77no7TkUts6PVryAgPyBi1/img.png)

# 11. S3 Batch Operations

- 단일 요청으로 기존 S3 객체에서 대량 작업을 수행하는 서비스
    - 많은 S3 객체의 메타데이터와 프로퍼티 수정
    - S3 버킷간 객체 복사
    - 암호화되지 않은 모든 객체 암호화 가능
    - ACL, 태그 수정
    - S3 Glacier에서 한 번에 많은 객체 복원
    - 람다 함수를 호출하여 배치 연산의 각 객체에서 커스텀한 작업 수행 가능
- 객체 목록에서 원하는 작업은 무엇이든지 수행할 수 있다.
- 작업은 객체의 목록, 수행할 작업과 작업에 대한 옵션 매개 변수로 구성
- 직접 스크립트를 짜지 않고 배치 연산을 사용하는 이유
    - 재시도 관리
    - 진행사항 추적, 작업 완료 알림
    - 보고서 생성
- S3 Inventory라는 기능을 사용해 객체 목록을 불러오고 Select를 사용해 객체를 필터링
- S3 배치 작업에 수행할 작업, 매개변수와 함께 객체 목록 전달
- 배치작업 수행

유스케이스

- S3 Inventory를 사용해 암호화되지 않은 모든 객체를 찾는다
- Batch 연산을 통해 한 번에 모두 암호화

![](https://blog.kakaocdn.net/dn/bw9hdq/btshBlNzk0k/Fk6WjKT79fgZQkZSBFRCtk/img.png)


---


# 1. 객체 암호화

4가지 방법이 있다

## 1.1. SSE(서버사이드 암호화)

- Amazon S3 관리형 키를 사용 → SSE-S3
- KMS 키를 사용해서 암호화 키를 관리하는 → SSE-KMS
- 고객이 제공하는 키 → SSE-C

## 1.2. 클라이언트 암호화

- 클라이언트에서 암호화해서 업로드

어느 것이 어떤 상황에 해당하는지 이해해야한다

# 2. S3 SSE-S3

- AWS에서 처리, 관리 및 소유하는 키로 암호화를 진행
- 사용자는 접근 불가
- AWS에 의해 서버측에서 암호화 AES-256 방식
- 헤더를 "x-amz-server-side-encryption": "AES256"로 설정해서 SSE-S3 매커니즘으로 객체를 암호화하도록 S3에 요청해야한다

![](https://blog.kakaocdn.net/dn/dnkDUT/btshBltXNsH/VspKmqk1Fbg7gYdS2aLTc1/img.png)

# 3. SSE-KMS

- AWS에서 소유하는 키에 의존하는 대신 KMS(Key Management Service)로 자신의 키를 직접 관리하는 방식
- 사용자가 키를 제어 가능한다는 장점과 CloudTrail을 통해 사용량 감사 가능
    - KMS 키를 AWS에서 사용하면 여기서 발생하는 모든 일을 기록하는 역할을 담당하는 서비스가 CloudTrail
- • "x-amz-server-side-encryption": "aws:kms"
- 객체 자체에 대한 엑세스 뿐만 아니라 암호화하는데 사용한 AWS KMS 키에도 엑세스를 해야함
    - 보안이 한단계 추가

![](https://blog.kakaocdn.net/dn/xu3LX/btshBlHyEhX/6SDbYkG9rPzrAEXZRxK2P1/img.png)

## 3.1. SSE-KMS의 제한사항

- 업로드, 다운로드할 때 KMS키를 사용해야함
- 단순히 암호화 복호화 할때마다 API 호출이 필요함, KMS API는 초당 KMS 할당량에 포함된다
    - 리전에따라 5,500, ~ 30,000개를 처리할 수 있고 콘솔을 통해 늘릴 수 있다
- 처리량이 매우 높은 S3 버킷이 KMS로 암호화되었다면 쓰로틀링이 발생할 수 있다

![](https://blog.kakaocdn.net/dn/5s12d/btshVAW6pno/RIqo77ou3gktw2kB3NGck1/img.jpg)

# 4. SSE-C

- 키가 AWS 외부에서 관리되지만 AWS로 키를 보내기 때문에 서버 측 암호화
- 암호화 키는 AWS S3에 저장되지 않고 사용 후 폐기함
- HTTPS가 반드시 사용되어야하고 업로드, 다운로드시 HTTP 헤더에 키를 포함하여 전달해야함

![](https://blog.kakaocdn.net/dn/c4UvS2/btshCg0frWC/YHnNFVoXL8y43ABJKKhGKk/img.png)

# 5. 클라이언트 측 암호화

- 클라이언트 암호화로 간단하게 구현가능(Amazon S3 Client-Side Encryption Library)
- 데이터를 S3로 보내기전에 클라이언트가 직접 암호화해야함
- 복호화는 S3 외부의 클라이언트 측에서 실행
- 클라이언트가 전적으로 키와 암호화 주기를 관리해야한다

![](https://blog.kakaocdn.net/dn/qXcDw/btshE9M7h07/MPOENkfuPDBiVLSoikAYFK/img.png)

# 6. AWS 전송 중 암호화 (SSL/TLS)

- S3 버킷에는 두 개의 엔드포인트가 있다
    - HTTP, HTTPS
- S3를 사용하는 경우 HTTPS가 권장됨
- SSE-C의 경우 반드시 HTTPS가 반드시 필요함

# 7. 실습 참고

암호화는 객체의 특정 버전 ID에 적용된다

클라이언트 사이드 암호화는 S3 콘솔에서 실제 암호화되었는지 확인할 수 없다

# 8. 기본 암호화 vs 버킷 정책간 차이점

기본 암호화 → 내장 암호화 방식, 버킷 안에서만 암호화가 유효

버킷 정책 → 버킷에 대해 액세스 및 권한을 제어하는 데 사용

- 암호화를 강제할 수 있도록 하는 것이 버킷 정책
    - 정책을 설정한다면 지정된 암호화 헤더가 없는 S3 객체를 버킷에 넣는 API 호출을 거부한다
    - Condition 항목을 보면 된다
    - 오른쪽은 암호화 되지않은 객체만 강제시킨다
- 헤더가 AES-256과 같지 않으면 API 호출을 거부한다
- 버킷 정책을 사용하는 대신 암호화되지 않은 객체의 API 호출을 거부하도록 설정도 가능
    - 기본 암호화 옵션을 사용하면 객체가 암호화 되지 않은채로 업로드되도 S3에 의해 함호화됨
- 버킷 정책은 기본 암호화전에 평가된다
    - 따라서 기본 암호화, 버킷 정책 두가지 다 설정하고 암호화되지 않은 객체를 업로드하면 거부된다 → 기본 암호화 단계에 도달할 수 없기 때문임

![](https://blog.kakaocdn.net/dn/UsfbK/btshYcVW3MI/MckiOo7Er12Yl8e30fgs9k/img.png)

# 9. S3 CORS Cross-Origin Resourse Sharing (CORS)

- Origin = scheme (protocol) + host (domain) + port
    - [https://www.example.com:443](https://www.example.com/)
        - protocol ⇒ HTTP , Domain ⇒ [ww.exmaple.com](http://ww.exmaple.com/)
- 웹브라우저 기반 보안 매커니즘
- 메인 오리진을 방문하는 동안 다른 오리진에 대한 요청을 허용하거나 거부하는 정책 중 하나
    - Same origin: [http://example.com](http://example.com/)/app1 & [http://example.com](http://example.com/)/app2
    - Different origins: [http://www.example.com](http://www.example.com/) & [http://other.example.com](http://other.example.com/)
    - 프로토콜, 도메인, 포트가 동일할 때 오리진이 같다고 말한다
- 요청 프로토콜의 일부로 다른 웹사이트에 요청을 보내야 할 때 다른 오리진이 CORS 헤더를 사용해서 요청을 허용하지 않는 한 해당 요청은 이행되지 않는다
    - 이것을 Access-Control-Allow-Origin Header라고 한다
    - Preflight에서 CORS 헤더 확인 가능

![](https://blog.kakaocdn.net/dn/cGj8AG/btshG79tr4d/XzK7qKGDfFnEymPc1DR661/img.png)

# 10. Amazon S3에서 CORS - 시험에 자주 나오는 유형

- 클라이언트가 S3 버킷에서 cross origin 요청을 하면 정확한 CORS 헤더를 활성화해야 함
- 작업을 빠르게 수행 하려면 특정 오리진을 허용하거나 * (모든 오리진)을 허용

![](https://blog.kakaocdn.net/dn/eg1hrS/btshYbQg0vR/sSLWLIQD3KZvSvK7CKbOtK/img.png)

첫 HTML을 반환한 웹사이트가 Origin이 되고 이미지를 주는 버킷이 Host가 된다

그러니까 다른 오리진에 대한 요청을 받는 쪽이 Host이다 헷갈리는거 주의

Host가 되는 S3 버킷에도 CORS 헤더를 잘 설정해놔야 다른 오리진에서 S3 버킷에 있는 이미지, 파일을 요청할 수 있게 해준다

# 11. CORS 실습

S3 버킷 객체를 get하는 요청을 퍼블릭으로 만듬

![](https://blog.kakaocdn.net/dn/cK65Mz/btshCeuwlXs/0GD6d24k2iiyRB1EKEQjN1/img.png)

![](https://blog.kakaocdn.net/dn/3CSn8/btshCgFVsS2/fVAEnLJbA2ObIL1uPHKEBk/img.png)

# 12. S3 MFA Delete - 추가 보호 기능

- MFA란 멀티팩터 인증으로 사용자가 장치에서 코드를 생성하도록 강제
- MFA를 통해 S3에서 중요한 작업을 수행하기 전 S3에 코드를 삽입하도록 만들 수 있다
- MFA는 언제 필요할까
    - 객체 버전을 영구적으로 삭제할 때 → 영구 삭제에 대한 보호 설정
    - 버킷에서 버저닝을 중단할 때
    - 파괴적인 기능을 수행할 때 MFA 보호가 필요하다
- 버저닝 활성화, 삭제된 버전 나열은 파괴적인 작업이라고 보기 어려우므로 확인 절차가 굳이 필요하진 않다.
- 버킷 소유자, 루트 계정만이 MFA Delete를 활성/비활성화 할 수 있다

# 13. S3 Access Logs

- 감사 목적으로 버킷에 대한 모든 엑세스를 기록할 수 잇다
- 어떤 계정에서든 S3로 보내는 요청을 승인 또는 거부 여부와 상관없이 다른 S3버킷에 파일로 기록된다
- 데이터 분석 도구로 분석도 가능한데 대상 로깅 버킷은 서로 같은 리전에 있어야 한다
    - 모든 요청이 버킷에 기록하도록 설정하면 된다

![](https://blog.kakaocdn.net/dn/bb4ldD/btshCeVCdW3/YeDKdngyfVaOomqHb34nk0/img.png)

- 로그형식 확인 가능 [https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/LogFormat.html](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/LogFormat.html)

## 13.1. 엑세스로그 주의사항

- 로깅 버킷을 모니터링 되는 버킷과 동일하게 설정하면 안된다.
    - 로깅 루프가 설정되어 로깅 로그가 무한루프 걸린다
    - 요금도 많이 나오니 주의

# 14. S3 - 사전 서명된 URL (Pre-Signed URLs)

- 공용 이미지를 만드는 방법
- S3 콘솔,CLI, SDK를 사용하여 생성할 수 있는 URL
    - 만료기간이 있다 콘솔의 경우 최대 12시간
    - CLI의 경우 최대 168시간
- 미리 서명된 URL을 생성할 때 URL을 받는 사용자는 URL을 생성하는 사용자의 GET 또는 PUT에 대한 권한을 상속한다
- 프라이빗 버킷을 공개설정하지 않고 URL을 생성하여 해당 파일에 엑세스할 수 있는 URL을 제공한다 (노션 API를 사용하여 확인해봄)
- 특정 파일에 임시로 엑세스할 떄 많이 사용
- 예시
    - 로그인한 사용자만 버킷에서 프리미엄 비디오를 다운받을 수 있도록 함
    - 사용자 목록이 계속 변하는 경우 URL을 동적으로 생성해서 파일을 다운로드하면 접근에 대한 통제 가능
    - 일시적으로 사요자가 S3 버킷의 특정한 위치에 파일을 업로드하도록 허용할 수 있다

# 15. S3 Glacier Vault Lock(볼트 잠금)

- WORM 모델을 채용하기 위해 Glacier 볼트를 잠근다
    - WORM ( Write Once Read Many ) → 한번 쓰고 여러 번 읽는다
- 객체를 가져와서 S3 Glacier 볼트에 넣고 수정, 삭제 할 수 없도록 잠금
- 볼트 잠금 정책을 생성하고 편집을 위해 정책을 잠금 → 일단 설정하고 잠근 뒤에는 누구도 수정하고 삭제할 수 없다
    - 관리자나 AWS 서비스를 사용하더라도 삭제 불가
- 규정 준수나 데이터 보존같은 법률적인 사항에 매우 유용

![](https://blog.kakaocdn.net/dn/NJBet/btshVBaC5jQ/ZHgPikj1kCTyuzW7YcYk80/img.png)

# 16. S3 Object Lock - 두 가지 보존 모드의 차이점 중요

- 객체 잠금을 활성화 하려면 버저닝 활성화 필요
- WORM 모델 채택 가능 → 전체 버킷 수준이 아닌 모든 객체에 각각 적용할 수 있는 잠금
    - 단일객체 잠금 가능
- 특정 객체 버전이 특정 시간 동안 삭제되는 것을 차단 가능
- Retention mode (보존 모드) → Compliance Mode
    - 볼트 잠금과 비슷한데 그 누구도 덮어쓰거나 삭제할 수 없다.
    - 보존 모드, 보존 기간조차 줄일 수 없다
    - 엄격히 적용할 때 사용
- Retention mode (보존 모드) → Governance
    - 대부분의 사용자는 객체를 덮어쓰거나 삭제 하거나 로그 설정을 변경할 수 없다
    - 관리자 같은 일부 사용자는 IAM을 통한 특별 권한으로 보존 기간 변경이나 삭제 가능
    - 컴플라이언스보다 유연성이 있다 → 일부 권한을 갖고 삭제할 수 있음
- 두가지 모드 모두 보존 기간 설정 필요하며 고정된 기간동안 객체를 보호할 수 있다
- 원하는만큼 기간을 연장할 수 있다

#### 16.1.1.1. Legal Hold (법적 보존)

- 모든 객체를 무기한으로 보호 → 아주 중요한 객체에 보존을 설정 (재판에 사용될 수 있는 것들)
- 모드나 기간에 상관없이 영구 보존
- s3:PutObjectLegalHold라는 IAM 권한을 가진 사용자는 어떤 객체에든 법적 보존을 설정하거나 제거할 수 있다
- 유연한 모드로 객체를 보호하고 싶을 때 권한을 사용하고 법적 조사가 끝난 경우 법적 보존을 제거

# 17. S3 Access Points

- 그룹과 사용자 별로 버킷 정책 설정해야하는데 많으면 많을수록 더욱 복잡해진다
- 따라서 버킷 정책 대신 Access Points를 사용
    - 팀별로 Access Points를 설정해서 특정 팀이 엑세스 포인트를 통해 버킷에 엑세스 했을 때만 R/W를 할 수 있다
    - 특정 접두사에 대해 읽기/쓰기를 설정 가능
- 특정 정책을 엑세스 포인트마다 연결해서 최종적으로 S3 버킷에 엑세스 하는 방식을 관리하는 방식이다
- 엑세스 포인트마다 고유의 DNS와 정책이 있다
    - 특정 IAM/그룹으로 제한 가능
- 엑세스 포인트마다 하나의 정책을 가짐

![](https://blog.kakaocdn.net/dn/bz348d/btshE0W5cRy/s2UcsrdB9Bnps3KzAMzEt1/img.png)

# 18. S3 Object Lambda

- 버킷의 객체를 호출한 애플리케이션이 회수하기 전에 수정한다
- 쉽게 말해 객체에 대한 추가적인 처리를 수행하는 것
    - 객체가 S3로 업로드되거나 읽힐 때마다 실행되어 객체에 대한 추가적인 가공 또는 변형 작업
- S3 엑세스 포인트가 필요하다
- 사용 사례
    - 분석 애플리케이션의 경우 새 S3 버킷을 사용하는 것보단 람다 함수를 실행하는 Lambda 엑세스 포인트에 접근해 데이터를 가져와 코드를 실행시켜 가공된 데이터를 얻는다
    - 마케팅 앱의 경우 마찬가지로 다른 버킷을 만드는 것보단 Lambda를 통해 DB 데이터를 가져오는 작업을 수행해 가져온다
- S3 Object Lambda Access Point가 만들어진다는 것을 알아둬야한다
- Lambda를 서포팅하는 버킷의 S3 Access Point는 하나밖에 없다
- Use Cases
    - 분석이나 비프로덕션 환경을 위해 개인 식별 정보와 같은 Pll 데이터를 삭제해야하는 경우
    - XML에서 JSON으로 데이터 변환작업을 수행하는 경우
    - 상황에 맞춰 이미지 크기를 바꾸거나 워터마크를 남겨야하는 경우, 이때 워터마크는 요청한 사용자를 적용해야하는 동적인 요청인 경우

![](https://blog.kakaocdn.net/dn/bVJAxb/btshE0v00Ub/rFWgKo9bkal2SCigfS2mBk/img.png)